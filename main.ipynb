# ---------- Imports ----------
import pandas as pd
import numpy as np
import joblib
from google.colab import drive
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV

# ---------- Stage 1: Mount & Load Data ----------
drive.mount('/content/drive')
test_path = '/content/drive/MyDrive/ML/Project/test.csv'
train_path = '/content/drive/MyDrive/ML/Project/train.csv'

train = pd.read_csv(train_path)
test = pd.read_csv(test_path)

target_col = "WeightCategory"
X = train.drop(columns=[target_col])
y = train[target_col]

# ---------- Encode Target ----------
le = LabelEncoder()
y_encoded = le.fit_transform(y)
joblib.dump(le, "label_encoder.pkl")

# ---------- Preprocessing ----------
numeric_cols = X.select_dtypes(exclude=["object"]).columns.tolist()
categorical_cols = X.select_dtypes(include=["object"]).columns.tolist()

num_pipeline = Pipeline([("scaler", StandardScaler())])
cat_pipeline = Pipeline([("encoder", OneHotEncoder(handle_unknown="ignore"))])

preprocessor = ColumnTransformer([
    ("num", num_pipeline, numeric_cols),
    ("cat", cat_pipeline, categorical_cols)
])

X_processed = preprocessor.fit_transform(X)
X_test_processed = preprocessor.transform(test)

joblib.dump(preprocessor, "preprocessor.pkl")

print(f"Preprocessing complete! Train shape: {X_processed.shape}, Test shape: {X_test_processed.shape}")

# ---------- Train/Validation Split ----------
X_train, X_val, y_train, y_val = train_test_split(
    X_processed, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
)


# Base model (booster gbtree + early stopping)
xgb_model = XGBClassifier(
    objective="multi:softprob",
    num_class=len(np.unique(y_encoded)),
    tree_method="hist",
    eval_metric="mlogloss",
    random_state=42,
    early_stopping_rounds=150,
    gamma=0.8,
)

param_grid = {
    "max_depth": [7],
    "min_child_weight": [3, 4, 5],
    "subsample": [0.79, 0.8],
    "colsample_bytree": [0.78, 0.79],
    "learning_rate": [0.02, 0.018],
    "reg_alpha": [0.5, 0.48],
    "reg_lambda": [1.2, 1.5],
    "n_estimators": [5000, 6000],
}
model = GridSearchCV(
    estimator=xgb_model,
    param_grid=param_grid,
    cv=4,
    scoring='accuracy',
    verbose=2,
    n_jobs=-1
)

# Fit with early stopping
model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    verbose=False
)

# ---------- Evaluation ----------
y_pred_val = model.predict(X_val)
acc = accuracy_score(y_val, y_pred_val)

print(f"\nValidation Accuracy: {acc:.4f}")
print("\nClassification Report:\n", classification_report(y_val, y_pred_val))

# ---------- Save Model ----------
joblib.dump(model, "xgb_preprocessed_model.pkl")
print("\n Model saved as 'xgb_preprocessed.pkl'")

# ---------- Test Predictions ----------
test_preds = model.predict(X_test_processed)
test_labels = le.inverse_transform(test_preds)

if "id" in test.columns:
    ids = test["id"]
else:
    ids = np.arange(len(test))

submission = pd.DataFrame({
    "id": ids,
    "WeightCategory": test_labels
})

submission_path = "/content/drive/MyDrive/ML/Project/xgb_predictions.csv"
submission.to_csv(submission_path, index=False)

print(f"\nSubmission saved to: {submission_path}")

